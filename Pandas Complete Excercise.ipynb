{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "028f93ca",
   "metadata": {},
   "source": [
    "# Complete Pandas Mastery Notebook for Data Science & Machine Learning\n",
    "# ========================================================================\n",
    "\n",
    "PANDAS MASTERY NOTEBOOK\n",
    "=======================\n",
    "\n",
    "This comprehensive notebook covers all essential Pandas operations for Data Science and Machine Learning.\n",
    "Each section builds upon the previous one to create a complete learning path.\n",
    "\n",
    "RECOMMENDED DATASETS FOR PRACTICE:\n",
    "1. Kaggle Datasets: https://www.kaggle.com/datasets (Free account required)\n",
    "   - Titanic Dataset: https://www.kaggle.com/c/titanic/data\n",
    "   - House Prices: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
    "   - Iris Dataset: Built into sklearn\n",
    "   E.g: from sklearn.datasets import load_iris; \n",
    "        iris_df = pd.DataFrame(load_iris().data, columns=load_iris().feature_names)\n",
    "   \n",
    "2. UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets.php\n",
    "   - Adult Income Dataset\n",
    "   - Wine Quality Dataset\n",
    "   - Boston Housing Dataset\n",
    "\n",
    "3. Seaborn Built-in Datasets (accessible via seaborn.load_dataset())\n",
    "   - tips, flights, car_crashes, mpg, etc.\n",
    "    e.g:\n",
    "        tips_df = sns.load_dataset('tips')\n",
    "        flights_df = sns.load_dataset('flights')\n",
    "        car_crashes_df = sns.load_dataset('car_crashes')\n",
    "        mpg_df = sns.load_dataset('mpg')\n",
    "        titanic_df = sns.load_dataset('titanic')\n",
    "    \n",
    "    ### Additional popular datasets from seaborn library\n",
    "        penguins_df = sns.load_dataset('penguins')  # Palmer penguins data\n",
    "        diamonds_df = sns.load_dataset('diamonds')  # Diamond characteristics\n",
    "        exercise_df = sns.load_dataset('exercise')  # Exercise and diet data\n",
    "        fmri_df = sns.load_dataset('fmri')  # fMRI brain imaging data\n",
    "\n",
    "    ###Quick Setup for students\n",
    "    # Import required libraries first\n",
    "        import pandas as pd\n",
    "        import seaborn as sns\n",
    "        from sklearn.datasets import load_iris\n",
    "\n",
    "        # Then use any of the one-liners above\n",
    "\n",
    "4. Government Data: https://data.gov\n",
    "5. Google Dataset Search: https://datasetsearch.research.google.com\n",
    "\n",
    "For this notebook, we'll use one sample dataset to demonstrate different concepts. You are free to choose any dataset above and practice all the concepts based on this and my classroom examples like titanic and apple stock data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee4cf6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PANDAS MASTERY NOTEBOOK ===\n",
      "Version: Pandas 2.2.3\n",
      "NumPy Version: 2.1.3\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"=== PANDAS MASTERY NOTEBOOK ===\")\n",
    "print(\"Version: Pandas\", pd.__version__)\n",
    "print(\"NumPy Version:\", np.__version__)\n",
    "print(\"\\n\" + \"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b3b0ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä SECTION 1: DATA CREATION AND IMPORT/EXPORT\n",
      "--------------------------------------------------\n",
      "\n",
      "1.1 Creating DataFrames\n",
      "Employee DataFrame:\n",
      "      Name  Age      City  Salary Department\n",
      "0    Alice   25  New York   50000         IT\n",
      "1      Bob   30    London   60000    Finance\n",
      "2  Charlie   35     Paris   70000         IT\n",
      "3    Diana   28     Tokyo   55000         HR\n",
      "4      Eve   32    Sydney   65000    Finance\n",
      "\n",
      "1.2 Creating from lists of lists\n",
      "Products DataFrame:\n",
      "     Product  Quantity  Price\n",
      "0  Product A       100  25.50\n",
      "1  Product B       150  30.00\n",
      "2  Product C        75  18.75\n",
      "3  Product D       200  45.00\n",
      "\n",
      "1.3 Creating Series\n",
      "Temperature Series:\n",
      "Mon    20\n",
      "Tue    25\n",
      "Wed    30\n",
      "Thu    35\n",
      "Fri    40\n",
      "Name: Temperature, dtype: int64\n",
      "\n",
      "1.4 Creating date ranges\n",
      "Date DataFrame:\n",
      "        Date     Value\n",
      "0 2024-01-01 -0.348918\n",
      "1 2024-01-02 -0.743989\n",
      "2 2024-01-03 -1.079706\n",
      "3 2024-01-04  1.352312\n",
      "4 2024-01-05 -1.242399\n",
      "\n",
      "1.5 File I/O Operations (Examples)\n",
      "\n",
      "# Reading files\n",
      "df = pd.read_csv('data.csv')\n",
      "df = pd.read_excel('data.xlsx', sheet_name='Sheet1')\n",
      "df = pd.read_json('data.json')\n",
      "df = pd.read_sql('SELECT * FROM table', connection)\n",
      "\n",
      "# Writing files\n",
      "df.to_csv('output.csv', index=False)\n",
      "df.to_excel('output.xlsx', index=False, sheet_name='Data')\n",
      "df.to_json('output.json', orient='records')\n",
      "df.to_parquet('output.parquet')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 1: DATA CREATION AND IMPORT/EXPORT\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\nüìä SECTION 1: DATA CREATION AND IMPORT/EXPORT\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 1.1 Creating DataFrames from scratch\n",
    "print(\"\\n1.1 Creating DataFrames\")\n",
    "\n",
    "# Dictionary method\n",
    "data_dict = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'City': ['New York', 'London', 'Paris', 'Tokyo', 'Sydney'],\n",
    "    'Salary': [50000, 60000, 70000, 55000, 65000],\n",
    "    'Department': ['IT', 'Finance', 'IT', 'HR', 'Finance']\n",
    "}\n",
    "df_employees = pd.DataFrame(data_dict)\n",
    "print(\"Employee DataFrame:\")\n",
    "print(df_employees)\n",
    "\n",
    "# 1.2 Creating from lists\n",
    "print(\"\\n1.2 Creating from lists of lists\")\n",
    "data_list = [\n",
    "    ['Product A', 100, 25.50],\n",
    "    ['Product B', 150, 30.00],\n",
    "    ['Product C', 75, 18.75],\n",
    "    ['Product D', 200, 45.00]\n",
    "]\n",
    "df_products = pd.DataFrame(data_list, columns=['Product', 'Quantity', 'Price'])\n",
    "print(\"Products DataFrame:\")\n",
    "print(df_products)\n",
    "\n",
    "# 1.3 Creating Series\n",
    "print(\"\\n1.3 Creating Series\")\n",
    "s_temperatures = pd.Series([20, 25, 30, 35, 40], \n",
    "                          index=['Mon', 'Tue', 'Wed', 'Thu', 'Fri'],\n",
    "                          name='Temperature')\n",
    "print(\"Temperature Series:\")\n",
    "print(s_temperatures)\n",
    "\n",
    "# 1.4 Date range creation\n",
    "print(\"\\n1.4 Creating date ranges\")\n",
    "date_range = pd.date_range(start='2024-01-01', end='2024-01-10', freq='D')\n",
    "df_dates = pd.DataFrame({'Date': date_range, 'Value': np.random.randn(len(date_range))})\n",
    "print(\"Date DataFrame:\")\n",
    "print(df_dates.head())\n",
    "\n",
    "# 1.5 File operations examples (commented out - requires actual files)\n",
    "print(\"\\n1.5 File I/O Operations (Examples)\")\n",
    "print(\"\"\"\n",
    "# Reading files\n",
    "df = pd.read_csv('data.csv')\n",
    "df = pd.read_excel('data.xlsx', sheet_name='Sheet1')\n",
    "df = pd.read_json('data.json')\n",
    "df = pd.read_sql('SELECT * FROM table', connection)\n",
    "\n",
    "# Writing files\n",
    "df.to_csv('output.csv', index=False)\n",
    "df.to_excel('output.xlsx', index=False, sheet_name='Data')\n",
    "df.to_json('output.json', orient='records')\n",
    "df.to_parquet('output.parquet')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a21877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üîç SECTION 2: DATA EXPLORATION AND INSPECTION\n",
      "--------------------------------------------------\n",
      "\n",
      "2.1 Basic DataFrame Information\n",
      "Shape: (5, 5)\n",
      "Columns: ['Name', 'Age', 'City', 'Salary', 'Department']\n",
      "Data types:\n",
      "Name          object\n",
      "Age            int64\n",
      "City          object\n",
      "Salary         int64\n",
      "Department    object\n",
      "dtype: object\n",
      "Memory usage:\n",
      "Index         132\n",
      "Name          308\n",
      "Age            40\n",
      "City          315\n",
      "Salary         40\n",
      "Department    305\n",
      "dtype: int64\n",
      "\n",
      "2.2 Statistical Summary\n",
      "             Age       Salary\n",
      "count   5.000000      5.00000\n",
      "mean   30.000000  60000.00000\n",
      "std     3.807887   7905.69415\n",
      "min    25.000000  50000.00000\n",
      "25%    28.000000  55000.00000\n",
      "50%    30.000000  60000.00000\n",
      "75%    32.000000  65000.00000\n",
      "max    35.000000  70000.00000\n",
      "\n",
      "Info about the DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Name        5 non-null      object\n",
      " 1   Age         5 non-null      int64 \n",
      " 2   City        5 non-null      object\n",
      " 3   Salary      5 non-null      int64 \n",
      " 4   Department  5 non-null      object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 332.0+ bytes\n",
      "\n",
      "2.3 Viewing Data\n",
      "First 3 rows:\n",
      "      Name  Age      City  Salary Department\n",
      "0    Alice   25  New York   50000         IT\n",
      "1      Bob   30    London   60000    Finance\n",
      "2  Charlie   35     Paris   70000         IT\n",
      "\n",
      "Last 2 rows:\n",
      "    Name  Age    City  Salary Department\n",
      "3  Diana   28   Tokyo   55000         HR\n",
      "4    Eve   32  Sydney   65000    Finance\n",
      "\n",
      "Random sample (2 rows):\n",
      "  Name  Age    City  Salary Department\n",
      "4  Eve   32  Sydney   65000    Finance\n",
      "1  Bob   30  London   60000    Finance\n",
      "\n",
      "2.4 Unique Values and Counts\n",
      "Unique departments: ['IT' 'Finance' 'HR']\n",
      "Department counts:\n",
      "Department\n",
      "IT         2\n",
      "Finance    2\n",
      "HR         1\n",
      "Name: count, dtype: int64\n",
      "Number of unique cities: 5\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 2: DATA EXPLORATION AND INSPECTION\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\\nüîç SECTION 2: DATA EXPLORATION AND INSPECTION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 2.1 Basic information\n",
    "print(\"\\n2.1 Basic DataFrame Information\")\n",
    "print(f\"Shape: {df_employees.shape}\")\n",
    "print(f\"Columns: {list(df_employees.columns)}\")\n",
    "print(f\"Data types:\\n{df_employees.dtypes}\")\n",
    "print(f\"Memory usage:\\n{df_employees.memory_usage(deep=True)}\")\n",
    "\n",
    "# 2.2 Statistical summary\n",
    "print(\"\\n2.2 Statistical Summary\")\n",
    "print(df_employees.describe())\n",
    "print(\"\\nInfo about the DataFrame:\")\n",
    "df_employees.info()\n",
    "\n",
    "# 2.3 Viewing data\n",
    "print(\"\\n2.3 Viewing Data\")\n",
    "print(\"First 3 rows:\")\n",
    "print(df_employees.head(3))\n",
    "print(\"\\nLast 2 rows:\")\n",
    "print(df_employees.tail(2))\n",
    "print(\"\\nRandom sample (2 rows):\")\n",
    "print(df_employees.sample(2))\n",
    "\n",
    "# 2.4 Unique values and counts\n",
    "print(\"\\n2.4 Unique Values and Counts\")\n",
    "print(f\"Unique departments: {df_employees['Department'].unique()}\")\n",
    "print(f\"Department counts:\\n{df_employees['Department'].value_counts()}\")\n",
    "print(f\"Number of unique cities: {df_employees['City'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f311e9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üéØ SECTION 3: DATA SELECTION AND INDEXING\n",
      "--------------------------------------------------\n",
      "\n",
      "3.1 Column Selection\n",
      "Single column (Series):\n",
      "0      Alice\n",
      "1        Bob\n",
      "2    Charlie\n",
      "3      Diana\n",
      "4        Eve\n",
      "Name: Name, dtype: object\n",
      "\n",
      "Multiple columns (DataFrame):\n",
      "      Name  Salary\n",
      "0    Alice   50000\n",
      "1      Bob   60000\n",
      "2  Charlie   70000\n",
      "3    Diana   55000\n",
      "4      Eve   65000\n",
      "\n",
      "3.2 Row Selection\n",
      "Row by index (iloc):\n",
      "Name             Alice\n",
      "Age                 25\n",
      "City          New York\n",
      "Salary           50000\n",
      "Department          IT\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Multiple rows by index:\n",
      "      Name  Age    City  Salary Department\n",
      "1      Bob   30  London   60000    Finance\n",
      "2  Charlie   35   Paris   70000         IT\n",
      "3    Diana   28   Tokyo   55000         HR\n",
      "\n",
      "Row by label (loc):\n",
      "Name          Charlie\n",
      "Age                35\n",
      "City            Paris\n",
      "Salary          70000\n",
      "Department         IT\n",
      "Name: 2, dtype: object\n",
      "\n",
      "3.3 Conditional Selection\n",
      "Employees with salary > 60000:\n",
      "      Name  Age    City  Salary Department\n",
      "2  Charlie   35   Paris   70000         IT\n",
      "4      Eve   32  Sydney   65000    Finance\n",
      "\n",
      "IT Department employees:\n",
      "      Name  Age      City  Salary Department\n",
      "0    Alice   25  New York   50000         IT\n",
      "2  Charlie   35     Paris   70000         IT\n",
      "\n",
      "Young high earners (Age < 30 AND Salary > 50000):\n",
      "    Name  Age   City  Salary Department\n",
      "3  Diana   28  Tokyo   55000         HR\n",
      "\n",
      "3.4 Boolean Indexing\n",
      "Employees in New York or London:\n",
      "    Name  Age      City  Salary Department\n",
      "0  Alice   25  New York   50000         IT\n",
      "1    Bob   30    London   60000    Finance\n",
      "\n",
      "3.5 Query Method\n",
      "Query result (Age > 30 and Department == 'Finance'):\n",
      "  Name  Age    City  Salary Department\n",
      "4  Eve   32  Sydney   65000    Finance\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 3: DATA SELECTION AND INDEXING\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\\nüéØ SECTION 3: DATA SELECTION AND INDEXING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 3.1 Column selection\n",
    "print(\"\\n3.1 Column Selection\")\n",
    "print(\"Single column (Series):\")\n",
    "print(df_employees['Name'])\n",
    "print(\"\\nMultiple columns (DataFrame):\")\n",
    "print(df_employees[['Name', 'Salary']])\n",
    "\n",
    "# 3.2 Row selection\n",
    "print(\"\\n3.2 Row Selection\")\n",
    "print(\"Row by index (iloc):\")\n",
    "print(df_employees.iloc[0])\n",
    "print(\"\\nMultiple rows by index:\")\n",
    "print(df_employees.iloc[1:4])\n",
    "print(\"\\nRow by label (loc):\")\n",
    "print(df_employees.loc[2])\n",
    "\n",
    "# 3.3 Conditional selection\n",
    "print(\"\\n3.3 Conditional Selection\")\n",
    "high_salary = df_employees[df_employees['Salary'] > 60000]\n",
    "print(\"Employees with salary > 60000:\")\n",
    "print(high_salary)\n",
    "\n",
    "it_employees = df_employees[df_employees['Department'] == 'IT']\n",
    "print(\"\\nIT Department employees:\")\n",
    "print(it_employees)\n",
    "\n",
    "# Multiple conditions\n",
    "young_high_earners = df_employees[(df_employees['Age'] < 30) & (df_employees['Salary'] > 50000)]\n",
    "print(\"\\nYoung high earners (Age < 30 AND Salary > 50000):\")\n",
    "print(young_high_earners)\n",
    "\n",
    "# 3.4 Boolean indexing\n",
    "print(\"\\n3.4 Boolean Indexing\")\n",
    "mask = df_employees['City'].isin(['New York', 'London'])\n",
    "print(\"Employees in New York or London:\")\n",
    "print(df_employees[mask])\n",
    "\n",
    "# 3.5 Query method\n",
    "print(\"\\n3.5 Query Method\")\n",
    "result = df_employees.query(\"Age > 30 and Department == 'Finance'\")\n",
    "print(\"Query result (Age > 30 and Department == 'Finance'):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2228b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üßπ SECTION 4: DATA CLEANING AND PREPROCESSING\n",
      "--------------------------------------------------\n",
      "\n",
      "4.1 Handling Missing Values\n",
      "DataFrame with missing values:\n",
      "     A    B    C  D\n",
      "0  1.0  NaN  1.0  a\n",
      "1  2.0  2.0  2.0  b\n",
      "2  NaN  3.0  3.0  c\n",
      "3  4.0  4.0  NaN  d\n",
      "4  5.0  NaN  5.0  e\n",
      "\n",
      "Missing values count:\n",
      "A    1\n",
      "B    2\n",
      "C    1\n",
      "D    0\n",
      "dtype: int64\n",
      "Total missing values: 4\n",
      "\n",
      "After dropping rows with any missing values:\n",
      "     A    B    C  D\n",
      "1  2.0  2.0  2.0  b\n",
      "\n",
      "After dropping columns with any missing values:\n",
      "   D\n",
      "0  a\n",
      "1  b\n",
      "2  c\n",
      "3  d\n",
      "4  e\n",
      "\n",
      "Filling missing values with 0:\n",
      "     A    B    C  D\n",
      "0  1.0  0.0  1.0  a\n",
      "1  2.0  2.0  2.0  b\n",
      "2  0.0  3.0  3.0  c\n",
      "3  4.0  4.0  0.0  d\n",
      "4  5.0  0.0  5.0  e\n",
      "\n",
      "Forward fill:\n",
      "     A    B    C  D\n",
      "0  1.0  NaN  1.0  a\n",
      "1  2.0  2.0  2.0  b\n",
      "2  2.0  3.0  3.0  c\n",
      "3  4.0  4.0  3.0  d\n",
      "4  5.0  4.0  5.0  e\n",
      "\n",
      "Filling with column mean (numeric columns only):\n",
      "     A    B     C  D\n",
      "0  1.0  3.0  1.00  a\n",
      "1  2.0  2.0  2.00  b\n",
      "2  3.0  3.0  3.00  c\n",
      "3  4.0  4.0  2.75  d\n",
      "4  5.0  3.0  5.00  e\n",
      "\n",
      "Smart filling (different strategies for different data types):\n",
      "     A    B     C  D\n",
      "0  1.0  3.0  1.00  a\n",
      "1  2.0  2.0  2.00  b\n",
      "2  3.0  3.0  3.00  c\n",
      "3  4.0  4.0  2.75  d\n",
      "4  5.0  3.0  5.00  e\n",
      "\n",
      "4.2 Handling Duplicates\n",
      "DataFrame with duplicates:\n",
      "      Name  Age      City\n",
      "0    Alice   25  New York\n",
      "1      Bob   30    London\n",
      "2    Alice   25  New York\n",
      "3  Charlie   35     Paris\n",
      "\n",
      "Duplicate rows: 1\n",
      "After removing duplicates:\n",
      "      Name  Age      City\n",
      "0    Alice   25  New York\n",
      "1      Bob   30    London\n",
      "3  Charlie   35     Paris\n",
      "\n",
      "4.3 Data Type Conversion\n",
      "Original data types:\n",
      "Numbers       object\n",
      "Dates         object\n",
      "Categories    object\n",
      "dtype: object\n",
      "\n",
      "After conversion:\n",
      "Numbers                int64\n",
      "Dates         datetime64[ns]\n",
      "Categories          category\n",
      "dtype: object\n",
      "\n",
      "4.4 String Operations\n",
      "Original text:\n",
      "               Text\n",
      "0     Hello World  \n",
      "1     PYTHON pandas\n",
      "2     Data Science!\n",
      "3  machine learning\n",
      "\n",
      "After string operations:\n",
      "               Text           Cleaned  Length  Contains_Data\n",
      "0     Hello World         Hello World      15          False\n",
      "1     PYTHON pandas     Python Pandas      13          False\n",
      "2     Data Science!     Data Science!      13           True\n",
      "3  machine learning  Machine Learning      16          False\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 4: DATA CLEANING AND PREPROCESSING\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\\nüßπ SECTION 4: DATA CLEANING AND PREPROCESSING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 4.1 Creating sample data with missing values\n",
    "print(\"\\n4.1 Handling Missing Values\")\n",
    "df_dirty = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 2, 3, 4, np.nan],\n",
    "    'C': [1, 2, 3, np.nan, 5],\n",
    "    'D': ['a', 'b', 'c', 'd', 'e']\n",
    "})\n",
    "print(\"DataFrame with missing values:\")\n",
    "print(df_dirty)\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values count:\\n{df_dirty.isnull().sum()}\")\n",
    "print(f\"Total missing values: {df_dirty.isnull().sum().sum()}\")\n",
    "\n",
    "# Drop missing values\n",
    "print(\"\\nAfter dropping rows with any missing values:\")\n",
    "print(df_dirty.dropna())\n",
    "\n",
    "print(\"\\nAfter dropping columns with any missing values:\")\n",
    "print(df_dirty.dropna(axis=1))\n",
    "\n",
    "# Fill missing values\n",
    "print(\"\\nFilling missing values with 0:\")\n",
    "print(df_dirty.fillna(0))\n",
    "\n",
    "print(\"\\nForward fill:\")\n",
    "print(df_dirty.fillna(method='ffill'))\n",
    "\n",
    "print(\"\\nFilling with column mean (numeric columns only):\")\n",
    "# Fix: Only calculate mean for numeric columns\n",
    "numeric_cols = df_dirty.select_dtypes(include=[np.number]).columns\n",
    "df_filled = df_dirty.copy()\n",
    "df_filled[numeric_cols] = df_filled[numeric_cols].fillna(df_dirty[numeric_cols].mean())\n",
    "print(df_filled)\n",
    "\n",
    "# Alternative approach - fill each column appropriately\n",
    "print(\"\\nSmart filling (different strategies for different data types):\")\n",
    "df_smart_filled = df_dirty.copy()\n",
    "# Fill numeric columns with mean\n",
    "numeric_columns = df_smart_filled.select_dtypes(include=[np.number]).columns\n",
    "df_smart_filled[numeric_columns] = df_smart_filled[numeric_columns].fillna(df_smart_filled[numeric_columns].mean())\n",
    "# Fill categorical/string columns with mode or a default value\n",
    "categorical_columns = df_smart_filled.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    df_smart_filled[col] = df_smart_filled[col].fillna(df_smart_filled[col].mode()[0] if not df_smart_filled[col].mode().empty else 'Unknown')\n",
    "print(df_smart_filled)\n",
    "\n",
    "# 4.2 Duplicates\n",
    "print(\"\\n4.2 Handling Duplicates\")\n",
    "df_with_dupes = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Alice', 'Charlie'],\n",
    "    'Age': [25, 30, 25, 35],\n",
    "    'City': ['New York', 'London', 'New York', 'Paris']\n",
    "})\n",
    "print(\"DataFrame with duplicates:\")\n",
    "print(df_with_dupes)\n",
    "\n",
    "print(f\"\\nDuplicate rows: {df_with_dupes.duplicated().sum()}\")\n",
    "print(\"After removing duplicates:\")\n",
    "print(df_with_dupes.drop_duplicates())\n",
    "\n",
    "# 4.3 Data type conversion\n",
    "print(\"\\n4.3 Data Type Conversion\")\n",
    "df_convert = pd.DataFrame({\n",
    "    'Numbers': ['1', '2', '3', '4'],\n",
    "    'Dates': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04'],\n",
    "    'Categories': ['A', 'B', 'A', 'C']\n",
    "})\n",
    "print(\"Original data types:\")\n",
    "print(df_convert.dtypes)\n",
    "\n",
    "# Convert data types\n",
    "df_convert['Numbers'] = pd.to_numeric(df_convert['Numbers'])\n",
    "df_convert['Dates'] = pd.to_datetime(df_convert['Dates'])\n",
    "df_convert['Categories'] = df_convert['Categories'].astype('category')\n",
    "\n",
    "print(\"\\nAfter conversion:\")\n",
    "print(df_convert.dtypes)\n",
    "\n",
    "# 4.4 String operations\n",
    "print(\"\\n4.4 String Operations\")\n",
    "df_text = pd.DataFrame({'Text': ['  Hello World  ', 'PYTHON pandas', 'Data Science!', 'machine learning']})\n",
    "print(\"Original text:\")\n",
    "print(df_text)\n",
    "\n",
    "# String cleaning\n",
    "df_text['Cleaned'] = df_text['Text'].str.strip().str.lower().str.title()\n",
    "df_text['Length'] = df_text['Text'].str.len()\n",
    "df_text['Contains_Data'] = df_text['Text'].str.contains('Data', case=False)\n",
    "print(\"\\nAfter string operations:\")\n",
    "print(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93a9467a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üîß SECTION 5: DATA TRANSFORMATION AND MANIPULATION\n",
      "--------------------------------------------------\n",
      "\n",
      "5.1 Adding New Columns\n",
      "DataFrame with new columns:\n",
      "      Name  Age      City  Salary Department Salary_Category  Name_Length  \\\n",
      "0    Alice   25  New York   50000         IT             Low            5   \n",
      "1      Bob   30    London   60000    Finance          Medium            3   \n",
      "2  Charlie   35     Paris   70000         IT            High            7   \n",
      "3    Diana   28     Tokyo   55000         HR          Medium            5   \n",
      "4      Eve   32    Sydney   65000    Finance            High            3   \n",
      "\n",
      "  Age_Group  \n",
      "0     Young  \n",
      "1     Young  \n",
      "2    Middle  \n",
      "3     Young  \n",
      "4    Middle  \n",
      "\n",
      "5.2 Renaming Columns and Indices\n",
      "Renamed columns:\n",
      "['Employee_Name', 'Employee_Age', 'City', 'Salary', 'Department']\n",
      "\n",
      "5.3 Sorting Data\n",
      "Sorted by Salary (ascending):\n",
      "      Name  Age      City  Salary Department\n",
      "0    Alice   25  New York   50000         IT\n",
      "3    Diana   28     Tokyo   55000         HR\n",
      "1      Bob   30    London   60000    Finance\n",
      "4      Eve   32    Sydney   65000    Finance\n",
      "2  Charlie   35     Paris   70000         IT\n",
      "\n",
      "Sorted by Age (descending), then by Salary (ascending):\n",
      "      Name  Age      City  Salary Department\n",
      "2  Charlie   35     Paris   70000         IT\n",
      "4      Eve   32    Sydney   65000    Finance\n",
      "1      Bob   30    London   60000    Finance\n",
      "3    Diana   28     Tokyo   55000         HR\n",
      "0    Alice   25  New York   50000         IT\n",
      "\n",
      "5.4 Ranking\n",
      "With salary rankings:\n",
      "      Name  Salary  Salary_Rank\n",
      "0    Alice   50000          5.0\n",
      "1      Bob   60000          3.0\n",
      "2  Charlie   70000          1.0\n",
      "3    Diana   55000          4.0\n",
      "4      Eve   65000          2.0\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 5: DATA TRANSFORMATION AND MANIPULATION\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\\nüîß SECTION 5: DATA TRANSFORMATION AND MANIPULATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 5.1 Adding new columns\n",
    "print(\"\\n5.1 Adding New Columns\")\n",
    "df_employees_copy = df_employees.copy()\n",
    "df_employees_copy['Salary_Category'] = df_employees_copy['Salary'].apply(\n",
    "    lambda x: 'High' if x > 60000 else 'Medium' if x > 50000 else 'Low'\n",
    ")\n",
    "df_employees_copy['Name_Length'] = df_employees_copy['Name'].str.len()\n",
    "df_employees_copy['Age_Group'] = pd.cut(df_employees_copy['Age'], \n",
    "                                       bins=[0, 30, 40, 100], \n",
    "                                       labels=['Young', 'Middle', 'Senior'])\n",
    "print(\"DataFrame with new columns:\")\n",
    "print(df_employees_copy)\n",
    "\n",
    "# 5.2 Renaming columns and indices\n",
    "print(\"\\n5.2 Renaming Columns and Indices\")\n",
    "df_renamed = df_employees.rename(columns={'Name': 'Employee_Name', 'Age': 'Employee_Age'})\n",
    "print(\"Renamed columns:\")\n",
    "print(df_renamed.columns.tolist())\n",
    "\n",
    "# 5.3 Sorting\n",
    "print(\"\\n5.3 Sorting Data\")\n",
    "print(\"Sorted by Salary (ascending):\")\n",
    "print(df_employees.sort_values('Salary'))\n",
    "\n",
    "print(\"\\nSorted by Age (descending), then by Salary (ascending):\")\n",
    "print(df_employees.sort_values(['Age', 'Salary'], ascending=[False, True]))\n",
    "\n",
    "# 5.4 Ranking\n",
    "print(\"\\n5.4 Ranking\")\n",
    "df_employees_copy['Salary_Rank'] = df_employees_copy['Salary'].rank(ascending=False)\n",
    "print(\"With salary rankings:\")\n",
    "print(df_employees_copy[['Name', 'Salary', 'Salary_Rank']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b48c281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üìä SECTION 6: GROUPBY OPERATIONS AND AGGREGATIONS\n",
      "--------------------------------------------------\n",
      "\n",
      "6.1 Basic GroupBy Operations\n",
      "Average salary by department:\n",
      "Department\n",
      "Finance    62500.0\n",
      "HR         55000.0\n",
      "IT         60000.0\n",
      "Name: Salary, dtype: float64\n",
      "\n",
      "Multiple aggregations:\n",
      "             Salary                 Age      \n",
      "               mean    max    min  mean count\n",
      "Department                                   \n",
      "Finance     62500.0  65000  60000  31.0     2\n",
      "HR          55000.0  55000  55000  28.0     1\n",
      "IT          60000.0  70000  50000  30.0     2\n",
      "\n",
      "6.2 Complex Aggregations\n",
      "Sample sales data:\n",
      "        Date Product Region  Sales  Quantity\n",
      "0 2024-01-01       C   West    922         7\n",
      "1 2024-01-02       C  South    470        17\n",
      "2 2024-01-03       B   West    395         4\n",
      "3 2024-01-04       C   East    734         8\n",
      "4 2024-01-05       A  South    559        10\n",
      "\n",
      "Sales by Product and Region:\n",
      "Region   East  North  South  West\n",
      "Product                          \n",
      "A        4787   6087   4244  3499\n",
      "B        4963   4376   4313  2465\n",
      "C        5827   3241   4357  6929\n",
      "\n",
      "6.3 Custom Aggregation Functions\n",
      "Custom aggregations:\n",
      "         Sales                        Quantity          \n",
      "           sum        mean range_calc      sum       std\n",
      "Product                                                 \n",
      "A        18617  564.151515        873      354  5.185754\n",
      "B        16117  519.903226        869      286  4.897004\n",
      "C        20354  565.388889        847      342  5.848077\n",
      "\n",
      "6.4 Transform and Apply\n",
      "Sales with Z-scores by product:\n",
      "  Product  Sales  Sales_Zscore\n",
      "0       C    922      1.335317\n",
      "1       C    470     -0.357180\n",
      "2       B    395     -0.453947\n",
      "3       C    734      0.631358\n",
      "4       A    559     -0.022121\n",
      "5       C    430     -0.506959\n",
      "6       C    160     -1.517963\n",
      "7       B    360     -0.581151\n",
      "8       A    388     -0.756420\n",
      "9       A    680      0.497470\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 6: GROUPBY OPERATIONS AND AGGREGATIONS\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\\nüìä SECTION 6: GROUPBY OPERATIONS AND AGGREGATIONS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 6.1 Basic groupby\n",
    "print(\"\\n6.1 Basic GroupBy Operations\")\n",
    "dept_groups = df_employees.groupby('Department')\n",
    "print(\"Average salary by department:\")\n",
    "print(dept_groups['Salary'].mean())\n",
    "\n",
    "print(\"\\nMultiple aggregations:\")\n",
    "print(dept_groups.agg({\n",
    "    'Salary': ['mean', 'max', 'min'],\n",
    "    'Age': ['mean', 'count']\n",
    "}))\n",
    "\n",
    "# 6.2 Creating more complex sample data\n",
    "print(\"\\n6.2 Complex Aggregations\")\n",
    "sales_data = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=100, freq='D'),\n",
    "    'Product': np.random.choice(['A', 'B', 'C'], 100),\n",
    "    'Region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n",
    "    'Sales': np.random.randint(100, 1000, 100),\n",
    "    'Quantity': np.random.randint(1, 20, 100)\n",
    "})\n",
    "\n",
    "print(\"Sample sales data:\")\n",
    "print(sales_data.head())\n",
    "\n",
    "# Multiple groupby\n",
    "print(\"\\nSales by Product and Region:\")\n",
    "product_region_sales = sales_data.groupby(['Product', 'Region'])['Sales'].sum().unstack()\n",
    "print(product_region_sales)\n",
    "\n",
    "# 6.3 Custom aggregation functions\n",
    "print(\"\\n6.3 Custom Aggregation Functions\")\n",
    "def range_calc(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "custom_agg = sales_data.groupby('Product').agg({\n",
    "    'Sales': ['sum', 'mean', range_calc],\n",
    "    'Quantity': ['sum', 'std']\n",
    "})\n",
    "print(\"Custom aggregations:\")\n",
    "print(custom_agg)\n",
    "\n",
    "# 6.4 Transform and apply\n",
    "print(\"\\n6.4 Transform and Apply\")\n",
    "sales_data['Sales_Zscore'] = sales_data.groupby('Product')['Sales'].transform(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")\n",
    "print(\"Sales with Z-scores by product:\")\n",
    "print(sales_data[['Product', 'Sales', 'Sales_Zscore']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2cecd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üîó SECTION 7: MERGING, JOINING, AND CONCATENATING\n",
      "--------------------------------------------------\n",
      "\n",
      "7.1 Sample Data for Merging\n",
      "Customers:\n",
      "   CustomerID           Name      City\n",
      "0           1  Alice Johnson  New York\n",
      "1           2      Bob Smith    London\n",
      "2           3  Charlie Brown     Paris\n",
      "3           4   Diana Prince     Tokyo\n",
      "4           5      Eve Adams    Sydney\n",
      "\n",
      "Orders:\n",
      "   OrderID  CustomerID   Product  Amount\n",
      "0      101           1    Laptop    1000\n",
      "1      102           2     Mouse      25\n",
      "2      103           2  Keyboard      75\n",
      "3      104           3   Monitor     300\n",
      "4      105           4    Tablet     500\n",
      "5      106           6     Phone     800\n",
      "\n",
      "7.2 Different Types of Joins\n",
      "Inner Join:\n",
      "   CustomerID           Name      City  OrderID   Product  Amount\n",
      "0           1  Alice Johnson  New York      101    Laptop    1000\n",
      "1           2      Bob Smith    London      102     Mouse      25\n",
      "2           2      Bob Smith    London      103  Keyboard      75\n",
      "3           3  Charlie Brown     Paris      104   Monitor     300\n",
      "4           4   Diana Prince     Tokyo      105    Tablet     500\n",
      "\n",
      "Left Join:\n",
      "   CustomerID           Name      City  OrderID   Product  Amount\n",
      "0           1  Alice Johnson  New York    101.0    Laptop  1000.0\n",
      "1           2      Bob Smith    London    102.0     Mouse    25.0\n",
      "2           2      Bob Smith    London    103.0  Keyboard    75.0\n",
      "3           3  Charlie Brown     Paris    104.0   Monitor   300.0\n",
      "4           4   Diana Prince     Tokyo    105.0    Tablet   500.0\n",
      "5           5      Eve Adams    Sydney      NaN       NaN     NaN\n",
      "\n",
      "Right Join:\n",
      "   CustomerID           Name      City  OrderID   Product  Amount\n",
      "0           1  Alice Johnson  New York      101    Laptop    1000\n",
      "1           2      Bob Smith    London      102     Mouse      25\n",
      "2           2      Bob Smith    London      103  Keyboard      75\n",
      "3           3  Charlie Brown     Paris      104   Monitor     300\n",
      "4           4   Diana Prince     Tokyo      105    Tablet     500\n",
      "5           6            NaN       NaN      106     Phone     800\n",
      "\n",
      "Outer Join:\n",
      "   CustomerID           Name      City  OrderID   Product  Amount\n",
      "0           1  Alice Johnson  New York    101.0    Laptop  1000.0\n",
      "1           2      Bob Smith    London    102.0     Mouse    25.0\n",
      "2           2      Bob Smith    London    103.0  Keyboard    75.0\n",
      "3           3  Charlie Brown     Paris    104.0   Monitor   300.0\n",
      "4           4   Diana Prince     Tokyo    105.0    Tablet   500.0\n",
      "5           5      Eve Adams    Sydney      NaN       NaN     NaN\n",
      "6           6            NaN       NaN    106.0     Phone   800.0\n",
      "\n",
      "7.3 Concatenating DataFrames\n",
      "Vertical concatenation:\n",
      "   A  B\n",
      "0  1  3\n",
      "1  2  4\n",
      "2  5  7\n",
      "3  6  8\n",
      "\n",
      "Horizontal concatenation:\n",
      "   A  B   C   D\n",
      "0  1  3   9  11\n",
      "1  2  4  10  12\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 7: MERGING, JOINING, AND CONCATENATING\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\\nüîó SECTION 7: MERGING, JOINING, AND CONCATENATING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 7.1 Creating sample datasets for merging\n",
    "print(\"\\n7.1 Sample Data for Merging\")\n",
    "df_customers = pd.DataFrame({\n",
    "    'CustomerID': [1, 2, 3, 4, 5],\n",
    "    'Name': ['Alice Johnson', 'Bob Smith', 'Charlie Brown', 'Diana Prince', 'Eve Adams'],\n",
    "    'City': ['New York', 'London', 'Paris', 'Tokyo', 'Sydney']\n",
    "})\n",
    "\n",
    "df_orders = pd.DataFrame({\n",
    "    'OrderID': [101, 102, 103, 104, 105, 106],\n",
    "    'CustomerID': [1, 2, 2, 3, 4, 6],  # Note: CustomerID 6 doesn't exist in customers\n",
    "    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Tablet', 'Phone'],\n",
    "    'Amount': [1000, 25, 75, 300, 500, 800]\n",
    "})\n",
    "\n",
    "print(\"Customers:\")\n",
    "print(df_customers)\n",
    "print(\"\\nOrders:\")\n",
    "print(df_orders)\n",
    "\n",
    "# 7.2 Different types of joins\n",
    "print(\"\\n7.2 Different Types of Joins\")\n",
    "\n",
    "# Inner join\n",
    "inner_join = pd.merge(df_customers, df_orders, on='CustomerID', how='inner')\n",
    "print(\"Inner Join:\")\n",
    "print(inner_join)\n",
    "\n",
    "# Left join\n",
    "left_join = pd.merge(df_customers, df_orders, on='CustomerID', how='left')\n",
    "print(\"\\nLeft Join:\")\n",
    "print(left_join)\n",
    "\n",
    "# Right join\n",
    "right_join = pd.merge(df_customers, df_orders, on='CustomerID', how='right')\n",
    "print(\"\\nRight Join:\")\n",
    "print(right_join)\n",
    "\n",
    "# Outer join\n",
    "outer_join = pd.merge(df_customers, df_orders, on='CustomerID', how='outer')\n",
    "print(\"\\nOuter Join:\")\n",
    "print(outer_join)\n",
    "\n",
    "# 7.3 Concatenating DataFrames\n",
    "print(\"\\n7.3 Concatenating DataFrames\")\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
    "df3 = pd.DataFrame({'C': [9, 10], 'D': [11, 12]})\n",
    "\n",
    "print(\"Vertical concatenation:\")\n",
    "print(pd.concat([df1, df2], ignore_index=True))\n",
    "\n",
    "print(\"\\nHorizontal concatenation:\")\n",
    "print(pd.concat([df1, df3], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "135a28ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üîÑ SECTION 8: PIVOT TABLES AND RESHAPING\n",
      "--------------------------------------------------\n",
      "\n",
      "8.1 Pivot Tables\n",
      "Pivot table (Sales by Product and Region):\n",
      "Region   East  North  South  West\n",
      "Product                          \n",
      "A        4787   6087   4244  3499\n",
      "B        4963   4376   4313  2465\n",
      "C        5827   3241   4357  6929\n",
      "\n",
      "8.2 Melting Data\n",
      "Wide format:\n",
      "   ID  Jan  Feb  Mar\n",
      "0   1  100  110  120\n",
      "1   2  200  190  210\n",
      "2   3  150  160  140\n",
      "\n",
      "Long format (melted):\n",
      "   ID Month  Sales\n",
      "0   1   Jan    100\n",
      "1   2   Jan    200\n",
      "2   3   Jan    150\n",
      "3   1   Feb    110\n",
      "4   2   Feb    190\n",
      "5   3   Feb    160\n",
      "6   1   Mar    120\n",
      "7   2   Mar    210\n",
      "8   3   Mar    140\n",
      "\n",
      "8.3 Stacking and Unstacking\n",
      "Creating sample MultiIndex data without duplicates:\n",
      "MultiIndex Series (with unique index):\n",
      "Product  Region  ID\n",
      "C        West    0     922\n",
      "         South   1     470\n",
      "B        West    2     395\n",
      "C        East    3     734\n",
      "A        South   4     559\n",
      "C        North   5     430\n",
      "                 6     160\n",
      "B        South   7     360\n",
      "A        South   8     388\n",
      "                 9     680\n",
      "Name: Sales, dtype: int64\n",
      "\n",
      "Unstacked (to DataFrame):\n",
      "Region      East  North  South   West\n",
      "Product ID                           \n",
      "A       4    NaN    NaN  559.0    NaN\n",
      "        8    NaN    NaN  388.0    NaN\n",
      "        9    NaN    NaN  680.0    NaN\n",
      "        10   NaN  373.0    NaN    NaN\n",
      "        11   NaN    NaN    NaN  709.0\n",
      "\n",
      "8.3.1 Alternative: Handling Duplicates by Aggregating\n",
      "Aggregated sales (no duplicates):\n",
      "Product  Region\n",
      "A        East      4787\n",
      "         North     6087\n",
      "         South     4244\n",
      "         West      3499\n",
      "B        East      4963\n",
      "         North     4376\n",
      "         South     4313\n",
      "         West      2465\n",
      "C        East      5827\n",
      "         North     3241\n",
      "Name: Sales, dtype: int64\n",
      "\n",
      "Unstacked aggregated data:\n",
      "Region   East  North  South  West\n",
      "Product                          \n",
      "A        4787   6087   4244  3499\n",
      "B        4963   4376   4313  2465\n",
      "C        5827   3241   4357  6929\n",
      "\n",
      "8.3.2 Stacking Example\n",
      "Original DataFrame:\n",
      "   A  B  C\n",
      "X  1  4  7\n",
      "Y  2  5  8\n",
      "Z  3  6  9\n",
      "\n",
      "Stacked (DataFrame to Series):\n",
      "X  A    1\n",
      "   B    4\n",
      "   C    7\n",
      "Y  A    2\n",
      "   B    5\n",
      "   C    8\n",
      "Z  A    3\n",
      "   B    6\n",
      "   C    9\n",
      "dtype: int64\n",
      "\n",
      "Unstacked back to DataFrame:\n",
      "   A  B  C\n",
      "X  1  4  7\n",
      "Y  2  5  8\n",
      "Z  3  6  9\n",
      "\n",
      "8.3.3 MultiIndex Operations with Time Series\n",
      "Time series MultiIndex:\n",
      "Date        Product  Region\n",
      "2024-01-01  A        North     788\n",
      "                     South     465\n",
      "            B        North     468\n",
      "                     South     339\n",
      "2024-01-02  A        North     923\n",
      "                     South     216\n",
      "            B        North     721\n",
      "                     South     109\n",
      "Name: Sales, dtype: int64\n",
      "\n",
      "Unstack by Region:\n",
      "Region              North  South\n",
      "Date       Product              \n",
      "2024-01-01 A          788    465\n",
      "           B          468    339\n",
      "2024-01-02 A          923    216\n",
      "           B          721    109\n",
      "2024-01-03 A          189    998\n",
      "\n",
      "Unstack by Product:\n",
      "Product              A    B\n",
      "Date       Region          \n",
      "2024-01-01 North   788  468\n",
      "           South   465  339\n",
      "2024-01-02 North   923  721\n",
      "           South   216  109\n",
      "2024-01-03 North   189  909\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 8: PIVOT TABLES AND RESHAPING\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\\nüîÑ SECTION 8: PIVOT TABLES AND RESHAPING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 8.1 Pivot tables\n",
    "print(\"\\n8.1 Pivot Tables\")\n",
    "pivot_table = sales_data.pivot_table(\n",
    "    values='Sales',\n",
    "    index='Product',\n",
    "    columns='Region',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "print(\"Pivot table (Sales by Product and Region):\")\n",
    "print(pivot_table)\n",
    "\n",
    "# 8.2 Melting (wide to long)\n",
    "print(\"\\n8.2 Melting Data\")\n",
    "wide_data = pd.DataFrame({\n",
    "    'ID': [1, 2, 3],\n",
    "    'Jan': [100, 200, 150],\n",
    "    'Feb': [110, 190, 160],\n",
    "    'Mar': [120, 210, 140]\n",
    "})\n",
    "print(\"Wide format:\")\n",
    "print(wide_data)\n",
    "\n",
    "long_data = pd.melt(wide_data, id_vars=['ID'], var_name='Month', value_name='Sales')\n",
    "print(\"\\nLong format (melted):\")\n",
    "print(long_data)\n",
    "\n",
    "# 8.3 Stacking and unstacking\n",
    "print(\"\\n8.3 Stacking and Unstacking\")\n",
    "\n",
    "# Fix: Create MultiIndex without duplicates by using reset_index and adding a unique identifier\n",
    "print(\"Creating sample MultiIndex data without duplicates:\")\n",
    "sample_multi_data = sales_data.head(20).copy().reset_index(drop=True)\n",
    "sample_multi_data['ID'] = range(len(sample_multi_data))  # Add unique identifier\n",
    "multi_index_df = sample_multi_data.set_index(['Product', 'Region', 'ID'])['Sales']\n",
    "print(\"MultiIndex Series (with unique index):\")\n",
    "print(multi_index_df.head(10))\n",
    "\n",
    "print(\"\\nUnstacked (to DataFrame):\")\n",
    "unstacked_result = multi_index_df.unstack(level='Region')\n",
    "print(unstacked_result.head())\n",
    "\n",
    "# Alternative approach: Aggregate duplicates before unstacking\n",
    "print(\"\\n8.3.1 Alternative: Handling Duplicates by Aggregating\")\n",
    "# Group by Product and Region, then sum the sales to remove duplicates\n",
    "aggregated_sales = sales_data.groupby(['Product', 'Region'])['Sales'].sum()\n",
    "print(\"Aggregated sales (no duplicates):\")\n",
    "print(aggregated_sales.head(10))\n",
    "\n",
    "print(\"\\nUnstacked aggregated data:\")\n",
    "unstacked_aggregated = aggregated_sales.unstack(fill_value=0)\n",
    "print(unstacked_aggregated)\n",
    "\n",
    "# 8.3.2 Stacking example\n",
    "print(\"\\n8.3.2 Stacking Example\")\n",
    "# Create a simple DataFrame to demonstrate stacking\n",
    "stack_demo = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6],\n",
    "    'C': [7, 8, 9]\n",
    "}, index=['X', 'Y', 'Z'])\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(stack_demo)\n",
    "\n",
    "stacked = stack_demo.stack()\n",
    "print(\"\\nStacked (DataFrame to Series):\")\n",
    "print(stacked)\n",
    "\n",
    "print(\"\\nUnstacked back to DataFrame:\")\n",
    "print(stacked.unstack())\n",
    "\n",
    "# 8.3.3 Working with real MultiIndex data\n",
    "print(\"\\n8.3.3 MultiIndex Operations with Time Series\")\n",
    "# Create a proper time series MultiIndex example\n",
    "dates = pd.date_range('2024-01-01', periods=10, freq='D')\n",
    "products = ['A', 'B']\n",
    "regions = ['North', 'South']\n",
    "\n",
    "# Create combinations without duplicates\n",
    "time_series_data = []\n",
    "for date in dates[:5]:  # Use fewer dates to avoid too much output\n",
    "    for product in products:\n",
    "        for region in regions:\n",
    "            time_series_data.append({\n",
    "                'Date': date,\n",
    "                'Product': product,\n",
    "                'Region': region,\n",
    "                'Sales': np.random.randint(100, 1000)\n",
    "            })\n",
    "\n",
    "ts_df = pd.DataFrame(time_series_data)\n",
    "ts_multiindex = ts_df.set_index(['Date', 'Product', 'Region'])['Sales']\n",
    "\n",
    "print(\"Time series MultiIndex:\")\n",
    "print(ts_multiindex.head(8))\n",
    "\n",
    "print(\"\\nUnstack by Region:\")\n",
    "ts_unstacked = ts_multiindex.unstack(level='Region')\n",
    "print(ts_unstacked.head())\n",
    "\n",
    "print(\"\\nUnstack by Product:\")\n",
    "ts_unstacked_product = ts_multiindex.unstack(level='Product')\n",
    "print(ts_unstacked_product.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f71b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "‚è∞ SECTION 9: TIME SERIES ANALYSIS\n",
      "--------------------------------------------------\n",
      "\n",
      "9.1 Time Series Data Creation\n",
      "Time series data (first 10 rows):\n",
      "                 Value  Volume\n",
      "Date                          \n",
      "2024-01-01  100.349434     145\n",
      "2024-01-02  100.602539     134\n",
      "2024-01-03  101.914528      70\n",
      "2024-01-04  102.541306     196\n",
      "2024-01-05  102.176417     187\n",
      "2024-01-06  101.481919      52\n",
      "2024-01-07  102.127722     154\n",
      "2024-01-08  100.733467      68\n",
      "2024-01-09   99.663383     111\n",
      "2024-01-10   99.004139     169\n",
      "\n",
      "9.2 Date/Time Operations\n",
      "With date components:\n",
      "                 Value  Volume  Year  Month  DayOfWeek  IsWeekend\n",
      "Date                                                             \n",
      "2024-01-01  100.349434     145  2024      1          0      False\n",
      "2024-01-02  100.602539     134  2024      1          1      False\n",
      "2024-01-03  101.914528      70  2024      1          2      False\n",
      "2024-01-04  102.541306     196  2024      1          3      False\n",
      "2024-01-05  102.176417     187  2024      1          4      False\n",
      "\n",
      "9.3 Resampling\n",
      "Monthly averages:\n",
      "Date\n",
      "2024-01-31    100.707466\n",
      "2024-02-29    102.297117\n",
      "2024-03-31    101.132374\n",
      "2024-04-30    100.858858\n",
      "2024-05-31    100.854285\n",
      "Freq: ME, Name: Value, dtype: float64\n",
      "\n",
      "Weekly volume sums:\n",
      "Date\n",
      "2024-01-07    938\n",
      "2024-01-14    935\n",
      "2024-01-21    694\n",
      "2024-01-28    836\n",
      "2024-02-04    790\n",
      "Freq: W-SUN, Name: Volume, dtype: int64\n",
      "\n",
      "9.4 Rolling Operations\n",
      "With rolling statistics:\n",
      "                 Value   Value_MA7  Value_MA30  Value_Std7\n",
      "Date                                                      \n",
      "2024-01-01  100.349434         NaN         NaN         NaN\n",
      "2024-01-02  100.602539         NaN         NaN         NaN\n",
      "2024-01-03  101.914528         NaN         NaN         NaN\n",
      "2024-01-04  102.541306         NaN         NaN         NaN\n",
      "2024-01-05  102.176417         NaN         NaN         NaN\n",
      "2024-01-06  101.481919         NaN         NaN         NaN\n",
      "2024-01-07  102.127722  101.599124         NaN    0.833518\n",
      "2024-01-08  100.733467  101.653985         NaN    0.745551\n",
      "2024-01-09   99.663383  101.519820         NaN    1.005488\n",
      "2024-01-10   99.004139  101.104050         NaN    1.355780\n",
      "2024-01-11   99.867509  100.722079         NaN    1.256374\n",
      "2024-01-12   98.668143  100.220897         NaN    1.279071\n",
      "2024-01-13   97.946261   99.715803         NaN    1.391288\n",
      "2024-01-14   98.190762   99.153380         NaN    0.992325\n",
      "2024-01-15   99.561907   98.986015         NaN    0.750822\n",
      "2024-01-16   98.936785   98.882215         NaN    0.689272\n",
      "2024-01-17   99.614337   98.969386         NaN    0.743698\n",
      "2024-01-18  100.617391   99.076512         NaN    0.926236\n",
      "2024-01-19  100.274535   99.305997         NaN    1.003937\n",
      "2024-01-20  100.942521   99.734034         NaN    0.965587\n",
      "2024-01-21  102.876443  100.403417         NaN    1.287807\n",
      "2024-01-22  101.647101  100.701302         NaN    1.301803\n",
      "2024-01-23  100.348448  100.902968         NaN    1.071949\n",
      "2024-01-24  100.751270  101.065387         NaN    0.919441\n",
      "2024-01-25  100.774606  101.087846         NaN    0.908529\n",
      "2024-01-26  100.082658  101.060435         NaN    0.939523\n",
      "2024-01-27  101.101586  101.083159         NaN    0.938118\n",
      "2024-01-28  101.422139  100.875401         NaN    0.559359\n",
      "2024-01-29  101.759089  100.891399         NaN    0.586072\n",
      "2024-01-30  102.658443  101.221399  100.621226    0.829280\n",
      "2024-01-31  103.294648  101.584738  100.719400    1.101474\n",
      "2024-02-01  104.458267  102.110976  100.847924    1.468664\n",
      "2024-02-02  105.487948  102.883160  100.967038    1.635941\n",
      "2024-02-03  103.997841  103.296911  101.015589    1.467879\n",
      "2024-02-04  103.959741  103.659425  101.075033    1.220155\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 9: TIME SERIES ANALYSIS\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\\n‚è∞ SECTION 9: TIME SERIES ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 9.1 Creating time series data\n",
    "print(\"\\n9.1 Time Series Data Creation\")\n",
    "dates = pd.date_range('2024-01-01', periods=365, freq='D')\n",
    "ts_data = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Value': np.random.randn(365).cumsum() + 100,\n",
    "    'Volume': np.random.randint(50, 200, 365)\n",
    "})\n",
    "ts_data.set_index('Date', inplace=True)\n",
    "print(\"Time series data (first 10 rows):\")\n",
    "print(ts_data.head(10))\n",
    "\n",
    "# 9.2 Date/time operations\n",
    "print(\"\\n9.2 Date/Time Operations\")\n",
    "ts_data['Year'] = ts_data.index.year\n",
    "ts_data['Month'] = ts_data.index.month\n",
    "ts_data['DayOfWeek'] = ts_data.index.dayofweek\n",
    "ts_data['IsWeekend'] = ts_data.index.dayofweek >= 5\n",
    "\n",
    "print(\"With date components:\")\n",
    "print(ts_data.head())\n",
    "\n",
    "# 9.3 Resampling\n",
    "print(\"\\n9.3 Resampling\")\n",
    "monthly_avg = ts_data.resample('M')['Value'].mean()\n",
    "print(\"Monthly averages:\")\n",
    "print(monthly_avg.head())\n",
    "\n",
    "weekly_sum = ts_data.resample('W')['Volume'].sum()\n",
    "print(\"\\nWeekly volume sums:\")\n",
    "print(weekly_sum.head())\n",
    "\n",
    "# 9.4 Rolling operations\n",
    "print(\"\\n9.4 Rolling Operations\")\n",
    "ts_data['Value_MA7'] = ts_data['Value'].rolling(window=7).mean()\n",
    "ts_data['Value_MA30'] = ts_data['Value'].rolling(window=30).mean()\n",
    "ts_data['Value_Std7'] = ts_data['Value'].rolling(window=7).std()\n",
    "\n",
    "print(\"With rolling statistics:\")\n",
    "print(ts_data[['Value', 'Value_MA7', 'Value_MA30', 'Value_Std7']].head(35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82be0bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üöÄ SECTION 10: ADVANCED OPERATIONS\n",
      "--------------------------------------------------\n",
      "\n",
      "10.1 Window Functions\n",
      "Window functions:\n",
      "  Group  Value  Cumulative_Sum  Rank  Pct_Change\n",
      "0     A     10              10   1.0         NaN\n",
      "1     A     20              30   2.0    1.000000\n",
      "2     A     30              60   3.0    0.500000\n",
      "3     B     15              15   1.0         NaN\n",
      "4     B     25              40   2.0    0.666667\n",
      "5     B     35              75   3.0    0.400000\n",
      "\n",
      "10.2 Categorical Data\n",
      "Ordered categorical data:\n",
      "  Grade  Score\n",
      "5     D     65\n",
      "2     C     78\n",
      "6     C     82\n",
      "1     B     87\n",
      "4     B     88\n",
      "0     A     95\n",
      "3     A     92\n",
      "7     A     94\n",
      "\n",
      "10.3 MultiIndex Operations\n",
      "MultiIndex DataFrame:\n",
      "                Value1    Value2\n",
      "First Second                    \n",
      "A     X      -1.060933  1.526926\n",
      "      Y      -0.290549 -0.802844\n",
      "B     X      -0.156097  0.850366\n",
      "      Y      -0.655960  0.843614\n",
      "\n",
      "Accessing level 'A':\n",
      "          Value1    Value2\n",
      "Second                    \n",
      "X      -1.060933  1.526926\n",
      "Y      -0.290549 -0.802844\n",
      "\n",
      "10.4 Performance Optimization Examples\n",
      "\n",
      "# Use vectorized operations instead of loops\n",
      "df['new_col'] = df['col1'] * df['col2']  # Good\n",
      "# df['new_col'] = df.apply(lambda x: x['col1'] * x['col2'], axis=1)  # Slower\n",
      "\n",
      "# Use loc/iloc for specific indexing\n",
      "df.loc[df['col'] > 5, 'new_col'] = 'High'  # Good\n",
      "\n",
      "# Use query for complex filtering\n",
      "df.query('col1 > 5 and col2 < 10')  # Often faster than boolean indexing\n",
      "\n",
      "# Use categorical for repeated string data\n",
      "df['category'] = df['category'].astype('category')\n",
      "\n",
      "# Use appropriate data types\n",
      "df['int_col'] = df['int_col'].astype('int32')  # Instead of int64 if possible\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 10: ADVANCED OPERATIONS\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\\nüöÄ SECTION 10: ADVANCED OPERATIONS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 10.1 Window functions\n",
    "print(\"\\n10.1 Window Functions\")\n",
    "df_window = pd.DataFrame({\n",
    "    'Group': ['A', 'A', 'A', 'B', 'B', 'B'],\n",
    "    'Value': [10, 20, 30, 15, 25, 35]\n",
    "})\n",
    "\n",
    "df_window['Cumulative_Sum'] = df_window.groupby('Group')['Value'].cumsum()\n",
    "df_window['Rank'] = df_window.groupby('Group')['Value'].rank()\n",
    "df_window['Pct_Change'] = df_window.groupby('Group')['Value'].pct_change()\n",
    "\n",
    "print(\"Window functions:\")\n",
    "print(df_window)\n",
    "\n",
    "# 10.2 Working with categorical data\n",
    "print(\"\\n10.2 Categorical Data\")\n",
    "df_cat = pd.DataFrame({\n",
    "    'Grade': ['A', 'B', 'C', 'A', 'B', 'D', 'C', 'A'],\n",
    "    'Score': [95, 87, 78, 92, 88, 65, 82, 94]\n",
    "})\n",
    "\n",
    "# Convert to categorical with order\n",
    "df_cat['Grade'] = pd.Categorical(df_cat['Grade'], \n",
    "                                categories=['D', 'C', 'B', 'A'], \n",
    "                                ordered=True)\n",
    "print(\"Ordered categorical data:\")\n",
    "print(df_cat.sort_values('Grade'))\n",
    "\n",
    "# 10.3 MultiIndex operations\n",
    "print(\"\\n10.3 MultiIndex Operations\")\n",
    "arrays = [\n",
    "    ['A', 'A', 'B', 'B'],\n",
    "    ['X', 'Y', 'X', 'Y']\n",
    "]\n",
    "tuples = list(zip(*arrays))\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=['First', 'Second'])\n",
    "\n",
    "df_multi = pd.DataFrame(np.random.randn(4, 2), index=index, columns=['Value1', 'Value2'])\n",
    "print(\"MultiIndex DataFrame:\")\n",
    "print(df_multi)\n",
    "\n",
    "print(\"\\nAccessing level 'A':\")\n",
    "print(df_multi.loc['A'])\n",
    "\n",
    "# 10.4 Performance optimization tips\n",
    "print(\"\\n10.4 Performance Optimization Examples\")\n",
    "print(\"\"\"\n",
    "# Use vectorized operations instead of loops\n",
    "df['new_col'] = df['col1'] * df['col2']  # Good\n",
    "# df['new_col'] = df.apply(lambda x: x['col1'] * x['col2'], axis=1)  # Slower\n",
    "\n",
    "# Use loc/iloc for specific indexing\n",
    "df.loc[df['col'] > 5, 'new_col'] = 'High'  # Good\n",
    "\n",
    "# Use query for complex filtering\n",
    "df.query('col1 > 5 and col2 < 10')  # Often faster than boolean indexing\n",
    "\n",
    "# Use categorical for repeated string data\n",
    "df['category'] = df['category'].astype('category')\n",
    "\n",
    "# Use appropriate data types\n",
    "df['int_col'] = df['int_col'].astype('int32')  # Instead of int64 if possible\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "645c4bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üìà SECTION 11: DATA VISUALIZATION WITH PANDAS\n",
      "--------------------------------------------------\n",
      "\n",
      "11.1 Basic Plotting Examples\n",
      "Creating sample visualization data...\n",
      "Sample plotting code:\n",
      "\n",
      "# Line plot\n",
      "plot_data.plot(x='x', y=['y1', 'y2'], kind='line', title='Line Plot')\n",
      "\n",
      "# Bar plot\n",
      "df_employees.plot(x='Name', y='Salary', kind='bar', title='Salary by Employee')\n",
      "\n",
      "# Histogram\n",
      "df_employees['Age'].plot(kind='hist', bins=10, title='Age Distribution')\n",
      "\n",
      "# Box plot\n",
      "sales_data.boxplot(column='Sales', by='Product')\n",
      "\n",
      "# Scatter plot\n",
      "plot_data.plot(x='y1', y='y2', kind='scatter', title='Scatter Plot')\n",
      "\n",
      "# Correlation heatmap (using seaborn)\n",
      "import seaborn as sns\n",
      "numeric_data = df_employees.select_dtypes(include=[np.number])\n",
      "sns.heatmap(numeric_data.corr(), annot=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 11: DATA VISUALIZATION WITH PANDAS\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\\nüìà SECTION 11: DATA VISUALIZATION WITH PANDAS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 11.1 Basic plotting\n",
    "print(\"\\n11.1 Basic Plotting Examples\")\n",
    "print(\"Creating sample visualization data...\")\n",
    "\n",
    "# Sample data for plotting\n",
    "plot_data = pd.DataFrame({\n",
    "    'x': range(10),\n",
    "    'y1': np.random.randn(10).cumsum(),\n",
    "    'y2': np.random.randn(10).cumsum(),\n",
    "    'category': np.random.choice(['A', 'B'], 10)\n",
    "})\n",
    "\n",
    "print(\"Sample plotting code:\")\n",
    "print(\"\"\"\n",
    "# Line plot\n",
    "plot_data.plot(x='x', y=['y1', 'y2'], kind='line', title='Line Plot')\n",
    "\n",
    "# Bar plot\n",
    "df_employees.plot(x='Name', y='Salary', kind='bar', title='Salary by Employee')\n",
    "\n",
    "# Histogram\n",
    "df_employees['Age'].plot(kind='hist', bins=10, title='Age Distribution')\n",
    "\n",
    "# Box plot\n",
    "sales_data.boxplot(column='Sales', by='Product')\n",
    "\n",
    "# Scatter plot\n",
    "plot_data.plot(x='y1', y='y2', kind='scatter', title='Scatter Plot')\n",
    "\n",
    "# Correlation heatmap (using seaborn)\n",
    "import seaborn as sns\n",
    "numeric_data = df_employees.select_dtypes(include=[np.number])\n",
    "sns.heatmap(numeric_data.corr(), annot=True)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "293ca643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ü§ñ SECTION 12: MACHINE LEARNING PREPROCESSING\n",
      "--------------------------------------------------\n",
      "\n",
      "12.1 Feature Engineering\n",
      "Feature engineered data:\n",
      "   age  income    education  experience  income_per_year_exp age_group  \\\n",
      "0   25   50000  High School           2         25000.000000     Young   \n",
      "1   30   60000     Bachelor           5         12000.000000     Young   \n",
      "2   35   70000       Master           8          8750.000000    Middle   \n",
      "3   40   80000          PhD          12          6666.666667    Middle   \n",
      "4   45   90000     Bachelor          15          6000.000000    Senior   \n",
      "5   50  100000  High School          20          5000.000000    Senior   \n",
      "\n",
      "   education_Bachelor  education_High School  education_Master  education_PhD  \n",
      "0               False                   True             False          False  \n",
      "1                True                  False             False          False  \n",
      "2               False                  False              True          False  \n",
      "3               False                  False             False           True  \n",
      "4                True                  False             False          False  \n",
      "5               False                   True             False          False  \n",
      "\n",
      "12.2 Train-Test Split Preparation\n",
      "\n",
      "# Prepare data for sklearn\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Separate features and target\n",
      "X = ml_data.drop(['target_column'], axis=1)  # Features\n",
      "y = ml_data['target_column']  # Target\n",
      "\n",
      "# Split the data\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "# Handle categorical variables\n",
      "X_train_encoded = pd.get_dummies(X_train)\n",
      "X_test_encoded = pd.get_dummies(X_test)\n",
      "\n",
      "# Ensure both sets have same columns\n",
      "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)\n",
      "\n",
      "\n",
      "12.3 Scaling and Normalization\n",
      "Original vs Scaled data:\n",
      "Original:\n",
      "   age  income  experience\n",
      "0   25   50000           2\n",
      "1   30   60000           5\n",
      "2   35   70000           8\n",
      "3   40   80000          12\n",
      "4   45   90000          15\n",
      "\n",
      "Scaled:\n",
      "       age   income  experience\n",
      "0 -1.46385 -1.46385   -1.372053\n",
      "1 -0.87831 -0.87831   -0.878114\n",
      "2 -0.29277 -0.29277   -0.384175\n",
      "3  0.29277  0.29277    0.274411\n",
      "4  0.87831  0.87831    0.768350\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 12: MACHINE LEARNING PREPROCESSING WITH PANDAS\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\\nü§ñ SECTION 12: MACHINE LEARNING PREPROCESSING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 12.1 Feature engineering\n",
    "print(\"\\n12.1 Feature Engineering\")\n",
    "ml_data = pd.DataFrame({\n",
    "    'age': [25, 30, 35, 40, 45, 50],\n",
    "    'income': [50000, 60000, 70000, 80000, 90000, 100000],\n",
    "    'education': ['High School', 'Bachelor', 'Master', 'PhD', 'Bachelor', 'High School'],\n",
    "    'experience': [2, 5, 8, 12, 15, 20]\n",
    "})\n",
    "\n",
    "# Create new features\n",
    "ml_data['income_per_year_exp'] = ml_data['income'] / ml_data['experience']\n",
    "ml_data['age_group'] = pd.cut(ml_data['age'], bins=[0, 30, 40, 100], labels=['Young', 'Middle', 'Senior'])\n",
    "\n",
    "# One-hot encoding\n",
    "education_dummies = pd.get_dummies(ml_data['education'], prefix='education')\n",
    "ml_data = pd.concat([ml_data, education_dummies], axis=1)\n",
    "\n",
    "print(\"Feature engineered data:\")\n",
    "print(ml_data)\n",
    "\n",
    "# 12.2 Train-test split preparation\n",
    "print(\"\\n12.2 Train-Test Split Preparation\")\n",
    "print(\"\"\"\n",
    "# Prepare data for sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target\n",
    "X = ml_data.drop(['target_column'], axis=1)  # Features\n",
    "y = ml_data['target_column']  # Target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle categorical variables\n",
    "X_train_encoded = pd.get_dummies(X_train)\n",
    "X_test_encoded = pd.get_dummies(X_test)\n",
    "\n",
    "# Ensure both sets have same columns\n",
    "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)\n",
    "\"\"\")\n",
    "\n",
    "# 12.3 Scaling and normalization\n",
    "print(\"\\n12.3 Scaling and Normalization\")\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Demonstrate scaling\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = ['age', 'income', 'experience']\n",
    "ml_data_scaled = ml_data.copy()\n",
    "ml_data_scaled[numeric_cols] = scaler.fit_transform(ml_data[numeric_cols])\n",
    "\n",
    "print(\"Original vs Scaled data:\")\n",
    "print(\"Original:\")\n",
    "print(ml_data[numeric_cols].head())\n",
    "print(\"\\nScaled:\")\n",
    "print(ml_data_scaled[numeric_cols].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42da2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "‚ö° SECTION 13: PERFORMANCE OPTIMIZATION AND BEST PRACTICES\n",
      "--------------------------------------------------\n",
      "\n",
      "13.1 Memory Optimization\n",
      "\n",
      "# Check memory usage\n",
      "df.info(memory_usage='deep')\n",
      "\n",
      "# Optimize data types\n",
      "df['int_col'] = df['int_col'].astype('int32')  # If values fit\n",
      "df['float_col'] = df['float_col'].astype('float32')  # If precision allows\n",
      "df['string_col'] = df['string_col'].astype('category')  # For repeated strings\n",
      "\n",
      "# Use sparse arrays for mostly zero data\n",
      "df['sparse_col'] = df['mostly_zero_col'].astype(pd.SparseDtype(\"float\", 0.0))\n",
      "\n",
      "\n",
      "13.2 Efficient Operations\n",
      "\n",
      "# Use vectorized operations\n",
      "df['result'] = df['col1'] * df['col2']  # Fast\n",
      "# Avoid: df['result'] = df.apply(lambda x: x['col1'] * x['col2'], axis=1)  # Slow\n",
      "\n",
      "# Use loc/iloc for indexing\n",
      "df.loc[mask, 'col'] = value  # Fast\n",
      "# Avoid: df[mask]['col'] = value  # Can be problematic\n",
      "\n",
      "# Use query for complex filtering\n",
      "result = df.query('col1 > 5 and col2 < 10 and col3 == \"value\"')  # Often faster\n",
      "\n",
      "# Chain operations efficiently\n",
      "result = (df\n",
      "    .query('sales > 1000')\n",
      "    .groupby('category')\n",
      "    .agg({'sales': 'sum', 'quantity': 'mean'})\n",
      "    .sort_values('sales', ascending=False)\n",
      ")\n",
      "\n",
      "\n",
      "13.3 Memory-Efficient Data Loading\n",
      "\n",
      "# Read large files in chunks\n",
      "chunk_size = 10000\n",
      "chunks = []\n",
      "for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):\n",
      "    # Process each chunk\n",
      "    processed_chunk = chunk.groupby('category').sum()\n",
      "    chunks.append(processed_chunk)\n",
      "result = pd.concat(chunks, ignore_index=True)\n",
      "\n",
      "# Specify data types when reading\n",
      "dtypes = {'col1': 'int32', 'col2': 'category', 'col3': 'float32'}\n",
      "df = pd.read_csv('file.csv', dtype=dtypes)\n",
      "\n",
      "# Read only necessary columns\n",
      "df = pd.read_csv('file.csv', usecols=['col1', 'col2', 'col3'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 13: PERFORMANCE OPTIMIZATION AND BEST PRACTICES\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\\n‚ö° SECTION 13: PERFORMANCE OPTIMIZATION AND BEST PRACTICES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n13.1 Memory Optimization\")\n",
    "print(\"\"\"\n",
    "# Check memory usage\n",
    "df.info(memory_usage='deep')\n",
    "\n",
    "# Optimize data types\n",
    "df['int_col'] = df['int_col'].astype('int32')  # If values fit\n",
    "df['float_col'] = df['float_col'].astype('float32')  # If precision allows\n",
    "df['string_col'] = df['string_col'].astype('category')  # For repeated strings\n",
    "\n",
    "# Use sparse arrays for mostly zero data\n",
    "df['sparse_col'] = df['mostly_zero_col'].astype(pd.SparseDtype(\"float\", 0.0))\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n13.2 Efficient Operations\")\n",
    "print(\"\"\"\n",
    "# Use vectorized operations\n",
    "df['result'] = df['col1'] * df['col2']  # Fast\n",
    "# Avoid: df['result'] = df.apply(lambda x: x['col1'] * x['col2'], axis=1)  # Slow\n",
    "\n",
    "# Use loc/iloc for indexing\n",
    "df.loc[mask, 'col'] = value  # Fast\n",
    "# Avoid: df[mask]['col'] = value  # Can be problematic\n",
    "\n",
    "# Use query for complex filtering\n",
    "result = df.query('col1 > 5 and col2 < 10 and col3 == \"value\"')  # Often faster\n",
    "\n",
    "# Chain operations efficiently\n",
    "result = (df\n",
    "    .query('sales > 1000')\n",
    "    .groupby('category')\n",
    "    .agg({'sales': 'sum', 'quantity': 'mean'})\n",
    "    .sort_values('sales', ascending=False)\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n13.3 Memory-Efficient Data Loading\")\n",
    "print(\"\"\"\n",
    "# Read large files in chunks\n",
    "chunk_size = 10000\n",
    "chunks = []\n",
    "for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):\n",
    "    # Process each chunk\n",
    "    processed_chunk = chunk.groupby('category').sum()\n",
    "    chunks.append(processed_chunk)\n",
    "result = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# Specify data types when reading\n",
    "dtypes = {'col1': 'int32', 'col2': 'category', 'col3': 'float32'}\n",
    "df = pd.read_csv('file.csv', dtype=dtypes)\n",
    "\n",
    "# Read only necessary columns\n",
    "df = pd.read_csv('file.csv', usecols=['col1', 'col2', 'col3'])\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1157f39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üí° SECTION 14: COMMON PANDAS PATTERNS AND TRICKS\n",
      "--------------------------------------------------\n",
      "\n",
      "14.1 Conditional Operations\n",
      "Conditional operations result:\n",
      "   score  subject grade   excellence\n",
      "0     85     Math     B         Good\n",
      "1     92  Science     A         Good\n",
      "2     78     Math     C         Good\n",
      "3     96  Science     A  Outstanding\n",
      "4     88     Math     B         Good\n",
      "5     71  Science     C         Good\n",
      "6     94     Math     A         Good\n",
      "\n",
      "14.2 Advanced Duplicate Handling\n",
      "Original data with duplicates:\n",
      "      name  age     city\n",
      "0    Alice   25       NY\n",
      "1      Bob   30       LA\n",
      "2    Alice   25       NY\n",
      "3  Charlie   35  Chicago\n",
      "4      Bob   31       LA\n",
      "\n",
      "Keep last duplicate:\n",
      "      name  age     city\n",
      "2    Alice   25       NY\n",
      "3  Charlie   35  Chicago\n",
      "4      Bob   31       LA\n",
      "\n",
      "Duplicate rows:\n",
      "    name  age city\n",
      "0  Alice   25   NY\n",
      "1    Bob   30   LA\n",
      "2  Alice   25   NY\n",
      "4    Bob   31   LA\n",
      "\n",
      "14.3 String Pattern Matching\n",
      "String pattern matching results:\n",
      "                 email           phone       domain phone_clean  is_gmail\n",
      "0      alice@gmail.com    123-456-7890    gmail.com  1234567890      True\n",
      "1        bob@yahoo.com  (555) 123-4567    yahoo.com  5551234567     False\n",
      "2  charlie@company.org    555.123.4567  company.org  5551234567     False\n",
      "3      diana@gmail.com    123 456 7890    gmail.com  1234567890      True\n",
      "\n",
      "14.4 Working with JSON-like Data\n",
      "Normalized JSON data:\n",
      "      name              skills\n",
      "0    Alice       [Python, SQL]\n",
      "1      Bob  [R, Excel, Python]\n",
      "2  Charlie      [Java, Python]\n",
      "\n",
      "Combined result:\n",
      "   id     name              skills\n",
      "0   1    Alice       [Python, SQL]\n",
      "1   2      Bob  [R, Excel, Python]\n",
      "2   3  Charlie      [Java, Python]\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 14: COMMON PANDAS PATTERNS AND TRICKS\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\\nüí° SECTION 14: COMMON PANDAS PATTERNS AND TRICKS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 14.1 Conditional operations\n",
    "print(\"\\n14.1 Conditional Operations\")\n",
    "sample_df = pd.DataFrame({\n",
    "    'score': [85, 92, 78, 96, 88, 71, 94],\n",
    "    'subject': ['Math', 'Science', 'Math', 'Science', 'Math', 'Science', 'Math']\n",
    "})\n",
    "\n",
    "# Using np.where\n",
    "sample_df['grade'] = np.where(sample_df['score'] >= 90, 'A',\n",
    "                     np.where(sample_df['score'] >= 80, 'B',\n",
    "                     np.where(sample_df['score'] >= 70, 'C', 'F')))\n",
    "\n",
    "# Using loc for conditional assignment\n",
    "sample_df.loc[sample_df['score'] >= 95, 'excellence'] = 'Outstanding'\n",
    "sample_df.loc[sample_df['score'] < 95, 'excellence'] = 'Good'\n",
    "\n",
    "print(\"Conditional operations result:\")\n",
    "print(sample_df)\n",
    "\n",
    "# 14.2 Working with duplicates advanced\n",
    "print(\"\\n14.2 Advanced Duplicate Handling\")\n",
    "df_dupes = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob'],\n",
    "    'age': [25, 30, 25, 35, 31],  # Bob has different ages\n",
    "    'city': ['NY', 'LA', 'NY', 'Chicago', 'LA']\n",
    "})\n",
    "\n",
    "print(\"Original data with duplicates:\")\n",
    "print(df_dupes)\n",
    "\n",
    "# Keep last occurrence\n",
    "print(\"\\nKeep last duplicate:\")\n",
    "print(df_dupes.drop_duplicates(subset=['name'], keep='last'))\n",
    "\n",
    "# Find duplicates\n",
    "print(\"\\nDuplicate rows:\")\n",
    "print(df_dupes[df_dupes.duplicated(subset=['name'], keep=False)])\n",
    "\n",
    "# 14.3 String pattern matching\n",
    "print(\"\\n14.3 String Pattern Matching\")\n",
    "text_df = pd.DataFrame({\n",
    "    'email': ['alice@gmail.com', 'bob@yahoo.com', 'charlie@company.org', 'diana@gmail.com'],\n",
    "    'phone': ['123-456-7890', '(555) 123-4567', '555.123.4567', '123 456 7890']\n",
    "})\n",
    "\n",
    "# Extract email domains\n",
    "text_df['domain'] = text_df['email'].str.extract(r'@(\\w+\\.\\w+)')\n",
    "\n",
    "# Extract phone numbers (digits only)\n",
    "text_df['phone_clean'] = text_df['phone'].str.replace(r'[^\\d]', '', regex=True)\n",
    "\n",
    "# Check for Gmail users\n",
    "text_df['is_gmail'] = text_df['email'].str.contains('gmail', case=False)\n",
    "\n",
    "print(\"String pattern matching results:\")\n",
    "print(text_df)\n",
    "\n",
    "# 14.4 Working with JSON-like data\n",
    "print(\"\\n14.4 Working with JSON-like Data\")\n",
    "json_data = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'info': [\n",
    "        {'name': 'Alice', 'skills': ['Python', 'SQL']},\n",
    "        {'name': 'Bob', 'skills': ['R', 'Excel', 'Python']},\n",
    "        {'name': 'Charlie', 'skills': ['Java', 'Python']}\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Normalize JSON data\n",
    "normalized = pd.json_normalize(json_data['info'])\n",
    "print(\"Normalized JSON data:\")\n",
    "print(normalized)\n",
    "\n",
    "# Combine with original DataFrame\n",
    "result = pd.concat([json_data[['id']], normalized], axis=1)\n",
    "print(\"\\nCombined result:\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "770cd5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üåç SECTION 15: REAL-WORLD SCENARIOS\n",
      "--------------------------------------------------\n",
      "\n",
      "15.1 Sales Analysis Scenario\n",
      "Sales data sample:\n",
      "        date   product region  quantity  unit_price salesperson  total_sales  \\\n",
      "0 2023-04-13    Laptop  North         6  601.084393       Diana  3606.506358   \n",
      "1 2024-03-11    Laptop  South         7  540.863466       Diana  3786.044262   \n",
      "2 2023-09-28    Laptop   West         3  770.837441       Alice  2312.512323   \n",
      "3 2023-04-17  Keyboard   West         7  318.371263         Bob  2228.598839   \n",
      "4 2023-03-13  Keyboard  South         8  385.397872         Bob  3083.182976   \n",
      "\n",
      "     month  \n",
      "0  2023-04  \n",
      "1  2024-03  \n",
      "2  2023-09  \n",
      "3  2023-04  \n",
      "4  2023-03  \n",
      "\n",
      "Top performing products by total sales:\n",
      "product\n",
      "Tablet      563951.036749\n",
      "Laptop      547566.827839\n",
      "Keyboard    532429.276217\n",
      "Monitor     493689.593250\n",
      "Mouse       475348.107952\n",
      "Name: total_sales, dtype: float64\n",
      "\n",
      "Monthly sales trend:\n",
      "month\n",
      "2023-01    136341.570655\n",
      "2023-02    119051.606408\n",
      "2023-03     94364.630159\n",
      "2023-04    156683.888313\n",
      "2023-05    118408.613459\n",
      "Freq: M, Name: total_sales, dtype: float64\n",
      "\n",
      "Salesperson performance by region:\n",
      "region                East          North          South           West\n",
      "salesperson                                                            \n",
      "Alice        117593.068155  131961.384454  185305.914010  185439.284020\n",
      "Bob          175481.963064  145408.438462  143333.940300  175483.194229\n",
      "Charlie      139560.953276  146764.033037  181023.585847  196100.834403\n",
      "Diana        150268.082089  180448.471621  196388.793616  162422.901423\n",
      "\n",
      "15.2 Customer Segmentation Scenario\n",
      "Customer segmentation sample:\n",
      "   customer_id  age  annual_income  spending_score  total_purchases  \\\n",
      "0            1   56   38700.361779              88               29   \n",
      "1            2   75   58464.247489              49               39   \n",
      "2            3   32   56629.705240              90                4   \n",
      "3            4   24   52835.317179              80                3   \n",
      "4            5   46   27312.072346              57               29   \n",
      "\n",
      "   avg_order_value income_tier age_group           clv  \n",
      "0       159.679707      Medium     51-65   4630.711503  \n",
      "1       437.100370      Medium       65+  17046.914446  \n",
      "2       454.137350      Medium     26-35   1816.549398  \n",
      "3        20.358900      Medium     18-25     61.076700  \n",
      "4       305.185468         Low     36-50   8850.378569  \n",
      "\n",
      "Customer segments by income and age:\n",
      "                       customer_id      clv  spending_score\n",
      "income_tier age_group                                      \n",
      "Low         18-25                7  6498.88           40.86\n",
      "            26-35               14  6730.57           49.14\n",
      "            36-50               17  7098.88           52.12\n",
      "            51-65               22  6301.59           51.73\n",
      "            65+                  9  8746.47           43.11\n",
      "Medium      18-25               31  6479.69           50.55\n",
      "            26-35               51  5830.91           58.20\n",
      "            36-50               72  6258.22           50.75\n",
      "            51-65               61  7717.90           47.93\n",
      "            65+                 66  6363.52           53.68\n",
      "High        18-25               18  4404.00           47.89\n",
      "            26-35               18  4823.68           52.06\n",
      "            36-50               31  8273.24           51.74\n",
      "            51-65               42  6182.48           57.86\n",
      "            65+                 35  6268.38           47.26\n",
      "Premium     18-25                0      NaN             NaN\n",
      "            26-35                0      NaN             NaN\n",
      "            36-50                0      NaN             NaN\n",
      "            51-65                0      NaN             NaN\n",
      "            65+                  1  2828.78           83.00\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 15: PANDAS WITH REAL-WORLD SCENARIOS\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\\nüåç SECTION 15: REAL-WORLD SCENARIOS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 15.1 Sales analysis scenario\n",
    "print(\"\\n15.1 Sales Analysis Scenario\")\n",
    "# Creating realistic sales data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2023-01-01', '2024-12-31', freq='D')\n",
    "products = ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Tablet']\n",
    "regions = ['North', 'South', 'East', 'West']\n",
    "\n",
    "sales_scenario = pd.DataFrame({\n",
    "    'date': np.random.choice(dates, 1000),\n",
    "    'product': np.random.choice(products, 1000),\n",
    "    'region': np.random.choice(regions, 1000),\n",
    "    'quantity': np.random.randint(1, 10, 1000),\n",
    "    'unit_price': np.random.uniform(50, 1000, 1000),\n",
    "    'salesperson': np.random.choice(['Alice', 'Bob', 'Charlie', 'Diana'], 1000)\n",
    "})\n",
    "\n",
    "sales_scenario['total_sales'] = sales_scenario['quantity'] * sales_scenario['unit_price']\n",
    "sales_scenario['month'] = sales_scenario['date'].dt.to_period('M')\n",
    "\n",
    "print(\"Sales data sample:\")\n",
    "print(sales_scenario.head())\n",
    "\n",
    "# Analysis examples\n",
    "print(\"\\nTop performing products by total sales:\")\n",
    "product_performance = sales_scenario.groupby('product')['total_sales'].sum().sort_values(ascending=False)\n",
    "print(product_performance)\n",
    "\n",
    "print(\"\\nMonthly sales trend:\")\n",
    "monthly_sales = sales_scenario.groupby('month')['total_sales'].sum()\n",
    "print(monthly_sales.head())\n",
    "\n",
    "print(\"\\nSalesperson performance by region:\")\n",
    "salesperson_region = sales_scenario.groupby(['salesperson', 'region'])['total_sales'].sum().unstack(fill_value=0)\n",
    "print(salesperson_region)\n",
    "\n",
    "# 15.2 Customer segmentation scenario\n",
    "print(\"\\n15.2 Customer Segmentation Scenario\")\n",
    "customer_data = pd.DataFrame({\n",
    "    'customer_id': range(1, 501),\n",
    "    'age': np.random.randint(18, 80, 500),\n",
    "    'annual_income': np.random.normal(50000, 20000, 500),\n",
    "    'spending_score': np.random.randint(1, 100, 500),\n",
    "    'total_purchases': np.random.randint(1, 50, 500),\n",
    "    'avg_order_value': np.random.uniform(20, 500, 500)\n",
    "})\n",
    "\n",
    "# Create customer segments\n",
    "customer_data['income_tier'] = pd.cut(customer_data['annual_income'], \n",
    "                                    bins=[0, 30000, 60000, 100000, float('inf')],\n",
    "                                    labels=['Low', 'Medium', 'High', 'Premium'])\n",
    "\n",
    "customer_data['age_group'] = pd.cut(customer_data['age'],\n",
    "                                  bins=[0, 25, 35, 50, 65, 100],\n",
    "                                  labels=['18-25', '26-35', '36-50', '51-65', '65+'])\n",
    "\n",
    "# Calculate customer lifetime value\n",
    "customer_data['clv'] = customer_data['total_purchases'] * customer_data['avg_order_value']\n",
    "\n",
    "print(\"Customer segmentation sample:\")\n",
    "print(customer_data.head())\n",
    "\n",
    "print(\"\\nCustomer segments by income and age:\")\n",
    "segment_analysis = customer_data.groupby(['income_tier', 'age_group']).agg({\n",
    "    'customer_id': 'count',\n",
    "    'clv': 'mean',\n",
    "    'spending_score': 'mean'\n",
    "}).round(2)\n",
    "print(segment_analysis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8bf5f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üêõ SECTION 16: ERROR HANDLING AND DEBUGGING\n",
      "--------------------------------------------------\n",
      "\n",
      "16.1 Common Pandas Errors and Solutions\n",
      "\n",
      "# KeyError: Column doesn't exist\n",
      "try:\n",
      "    result = df['non_existent_column']\n",
      "except KeyError as e:\n",
      "    print(f\"Column not found: {e}\")\n",
      "    print(\"Available columns:\", df.columns.tolist())\n",
      "\n",
      "# TypeError: Incompatible data types\n",
      "try:\n",
      "    result = df['string_col'] + df['numeric_col']\n",
      "except TypeError as e:\n",
      "    print(f\"Type error: {e}\")\n",
      "    # Convert types first\n",
      "    df['string_col'] = pd.to_numeric(df['string_col'], errors='coerce')\n",
      "\n",
      "# ValueError: Cannot convert data\n",
      "try:\n",
      "    df['date_col'] = pd.to_datetime(df['bad_date_col'])\n",
      "except ValueError as e:\n",
      "    print(f\"Date conversion error: {e}\")\n",
      "    # Use errors parameter\n",
      "    df['date_col'] = pd.to_datetime(df['bad_date_col'], errors='coerce')\n",
      "\n",
      "# SettingWithCopyWarning\n",
      "# Avoid: df[df['col'] > 5]['new_col'] = value\n",
      "# Use instead: df.loc[df['col'] > 5, 'new_col'] = value\n",
      "\n",
      "\n",
      "16.2 Debugging Techniques\n",
      "\n",
      "# Check data types\n",
      "print(df.dtypes)\n",
      "\n",
      "# Check for missing values\n",
      "print(df.isnull().sum())\n",
      "\n",
      "# Check unique values\n",
      "print(df['problematic_col'].unique())\n",
      "\n",
      "# Sample data for inspection\n",
      "print(df.sample(10))\n",
      "\n",
      "# Check memory usage\n",
      "print(df.info(memory_usage='deep'))\n",
      "\n",
      "# Validate data ranges\n",
      "print(df.describe())\n",
      "\n",
      "# Check for duplicates\n",
      "print(f\"Duplicates: {df.duplicated().sum()}\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 16: ERROR HANDLING AND DEBUGGING\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\\nüêõ SECTION 16: ERROR HANDLING AND DEBUGGING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n16.1 Common Pandas Errors and Solutions\")\n",
    "print(\"\"\"\n",
    "# KeyError: Column doesn't exist\n",
    "try:\n",
    "    result = df['non_existent_column']\n",
    "except KeyError as e:\n",
    "    print(f\"Column not found: {e}\")\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "\n",
    "# TypeError: Incompatible data types\n",
    "try:\n",
    "    result = df['string_col'] + df['numeric_col']\n",
    "except TypeError as e:\n",
    "    print(f\"Type error: {e}\")\n",
    "    # Convert types first\n",
    "    df['string_col'] = pd.to_numeric(df['string_col'], errors='coerce')\n",
    "\n",
    "# ValueError: Cannot convert data\n",
    "try:\n",
    "    df['date_col'] = pd.to_datetime(df['bad_date_col'])\n",
    "except ValueError as e:\n",
    "    print(f\"Date conversion error: {e}\")\n",
    "    # Use errors parameter\n",
    "    df['date_col'] = pd.to_datetime(df['bad_date_col'], errors='coerce')\n",
    "\n",
    "# SettingWithCopyWarning\n",
    "# Avoid: df[df['col'] > 5]['new_col'] = value\n",
    "# Use instead: df.loc[df['col'] > 5, 'new_col'] = value\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n16.2 Debugging Techniques\")\n",
    "print(\"\"\"\n",
    "# Check data types\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check unique values\n",
    "print(df['problematic_col'].unique())\n",
    "\n",
    "# Sample data for inspection\n",
    "print(df.sample(10))\n",
    "\n",
    "# Check memory usage\n",
    "print(df.info(memory_usage='deep'))\n",
    "\n",
    "# Validate data ranges\n",
    "print(df.describe())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4bd5e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üîß SECTION 17: INTEGRATION WITH OTHER LIBRARIES\n",
      "--------------------------------------------------\n",
      "\n",
      "17.1 Integration Examples\n",
      "\n",
      "# Pandas + NumPy\n",
      "import numpy as np\n",
      "df['log_values'] = np.log(df['positive_values'])\n",
      "df['sqrt_values'] = np.sqrt(df['values'])\n",
      "\n",
      "# Pandas + Scikit-learn\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Encode categorical variables\n",
      "le = LabelEncoder()\n",
      "df['category_encoded'] = le.fit_transform(df['category'])\n",
      "\n",
      "# Scale numerical features\n",
      "scaler = StandardScaler()\n",
      "df[['feature1', 'feature2']] = scaler.fit_transform(df[['feature1', 'feature2']])\n",
      "\n",
      "# Pandas + Matplotlib/Seaborn\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Direct plotting from pandas\n",
      "df.plot(kind='scatter', x='x_col', y='y_col')\n",
      "\n",
      "# Using seaborn with pandas\n",
      "sns.boxplot(data=df, x='category', y='value')\n",
      "\n",
      "# Pandas + Statsmodels\n",
      "import statsmodels.api as sm\n",
      "\n",
      "# Regression analysis\n",
      "X = df[['feature1', 'feature2']]\n",
      "y = df['target']\n",
      "X = sm.add_constant(X)  # Add intercept\n",
      "model = sm.OLS(y, X).fit()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# SECTION 17: INTEGRATION WITH OTHER LIBRARIES\n",
    "# ========================================================================\n",
    "\n",
    "print(\"\\n\\nüîß SECTION 17: INTEGRATION WITH OTHER LIBRARIES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n17.1 Integration Examples\")\n",
    "print(\"\"\"\n",
    "# Pandas + NumPy\n",
    "import numpy as np\n",
    "df['log_values'] = np.log(df['positive_values'])\n",
    "df['sqrt_values'] = np.sqrt(df['values'])\n",
    "\n",
    "# Pandas + Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "df['category_encoded'] = le.fit_transform(df['category'])\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "df[['feature1', 'feature2']] = scaler.fit_transform(df[['feature1', 'feature2']])\n",
    "\n",
    "# Pandas + Matplotlib/Seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Direct plotting from pandas\n",
    "df.plot(kind='scatter', x='x_col', y='y_col')\n",
    "\n",
    "# Using seaborn with pandas\n",
    "sns.boxplot(data=df, x='category', y='value')\n",
    "\n",
    "# Pandas + Statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Regression analysis\n",
    "X = df[['feature1', 'feature2']]\n",
    "y = df['target']\n",
    "X = sm.add_constant(X)  # Add intercept\n",
    "model = sm.OLS(y, X).fit()\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
